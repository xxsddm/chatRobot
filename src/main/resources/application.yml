server:
  port: 8080

spring:
  application:
    name: chat-service

# Chat model configuration
chat:
  model:
    provider: openai  # Default provider, can be changed to other providers
    api-key: ${OPENAI_API_KEY:5007dd8ce8cb40f69bde8508377c35e8.ESlHwMubfN6Gs8AD}
    base-url: ${OPENAI_BASE_URL:https://open.bigmodel.cn/api/paas/v4/chat/completions}
    model-name: ${MODEL_NAME:glm-4.5-flash}
    temperature: 0.7
    max-tokens: 1000
    timeout: 30s

logging:
  level:
    com.example.chat: DEBUG
    dev.langchain4j: DEBUG